## 注意事项



**网络层可视化**



[简易版教程](https://zhuanlan.zhihu.com/p/220403674)



[或者使用tensorboard](https://zhuanlan.zhihu.com/p/58961505)



**数据集合**



对数据集进行了Normalization处理



**代码注释**



读代码的时候注意多写注释 尤其是关键函数和一些比较难理解的函数



**调参/网络层**



不同参数结果对比 注意每次对比时需要有逻辑,比如为什么要改这些参数,改完以后可以印证什么or得出什么结论



除了调参 还可以尝试调整关键网络层



比如验证dropout



**结果分析**



重点回答任务书里面的几个问题



- 分析并列出影响各个模型实验结果的主要原因
- 列出不同算法的优劣 

- - FLOPs与时间复杂度
  - 对超参的敏感程度
  - 对数据量的需求



## 网络单元

**Encoder**

下采样层。



首先，使用步长为2的卷积层对图像进行步长为2的卷积运算。每次卷积后的图像通道数翻倍，大小减半。

这实际上是普通的卷积过程的简化版。可以考虑换成步长为1的卷积+一次池化。



之后按情况对数据进行InstanceNorm操作，我们了解到这个操作能使得每个batch的分布合理。



最后使用Dropout丢弃一部分输出（置零），达到正则化网络层的效果。我们验证了这一操作带来的效果



**Decoder**

上采样层。

首先是步长为2的转置卷积层，每次转置卷积后图像通道数减半，大小翻倍

此后的操作同样是归一化、Dropout正则化

## AutoEncoderGen 

## 背景

（AutoEncoder）自编码器是**表示学习**的经典算法，是一种无监督学习模型。其由一个encoder和一个decoder组成。前者将数据转化为一种不同的表现形式，后者将这个新的表示转化为原来的形式，转化的目的是让新的表示尽可能留有原来特性的同时，拥有各种好的特性。

我们将AutoEncoder应用于字体迁移任务中，构建了以此为基础的监督学习模型AutoEncoderGen。这一应用具有一定的合理性：目标字体和源字体整体变动不大，如果将目标字体理解为源字体的一种表示，在loss函数的引导下，生成器表示的源字体将朝目标字体靠近

### 原理分析

**模型架构**

![img](https://cdn.nlark.com/yuque/0/2022/png/26738067/1647400791944-e1181fc7-f98b-4ccd-b967-64dc20ebc391.png)

模型对于输入的图片，进行了如下操作：

连续6次Encoder操作，得到一个向量；其中最后三次Encoder时进行概率为0.5dropout处理，以实现对网络层的正则化。

再连续5次Decoder，重新得到一个长宽为原来两倍的矩阵；最后的final层将通道数、图像大小变回原样。







### loss函数

#### 生成器loss

loss函数是引导生成器“重表示”图像的关键。图像领域中L1误差较为常见。我们认为loss函数应该考虑如下几个因素：

* 图片灰度值两极分化（归一化后多为0和1），并且1的数量远大于0（图片以白为底）
* 字体迁移任务本身对结果的整体性要求较高
* 汉字的字体迁移具有一定的特殊性：不同的汉字之间的笔画数的差异可能很大，因此不同汉字与空白图像的L1距离的差异也会比较大。



**L1 loss**

$L1 \ loss$在图像处理领域非常常见，也是最简单的一类loss。`torch.nn.L1_loss`公式如下：
$$
L1\ {loss}(x,y) = \frac{\sum_n|x_i - y_i|}{n}
$$
我们认为其在汉字的字体迁移任务中可能存在不合理之处：

* 在大部分情况下，$mean(y)\approx 1$，在模型较差的情况下，生成器得到的$L1 \ loss$会小于空白图像。实际上我们在调整AutoEncoder的参数时，在学习率较高时，得到了一些输出空白图像的模型。
* loss值受笔划数影响较大



**fixed L1 loss**

针对L1 loss可能存在的局限性，我们尝试了修正的loss函数的计算方式$\text{fixed} \ L_1 \ loss$:
$$
\text{fixed} \ L1\ loss = \frac{L1\ loss(x,y)}{\text{mean} |1-y|+\alpha}
$$
其中$\alpha$是防止除以零错误的极小量。由于我们的输入数据中没有空白图像，$\alpha$ 设为0即可。$\text{fixed} \ L_1 \ loss$对于任何空白的输出的结果都为1；此外，该$\text{fixed} \ L_1 \ loss$下笔划较少的汉字的loss更敏感，我们期待它能够平衡不同汉字的loss。



#### 判别器误差







**模型分析**

理论上一个容量足够的自编码器的表示能力是很强的，因此我们的模型中有较多的Dropout操作防止矩阵过拟合。为了验证Dropout的效果，我们将除去模型中的所有Dropout。在我们的预期中，模型的train_loss将会下降，而test_loss将会上升。





### 运行结果

**摘要**

AutoEncoder在参数合适的情况下能够在训练集上取得较好的成效，但它的test_loss却持续处于较高的值

，但在大量的参数尝试下，AutoEncoder的test_loss依旧无法收敛，并一致持续在较高的值。

![img](https://cdn.nlark.com/yuque/0/2022/png/26738067/1647829444726-63b95746-b2da-4174-9055-65232d2b0cea.png)

如表所示，lr=0.0001，bs=8；lr=0.0005，bs=40时，训练期间的loss均可下降到较低的值；但test_loss几乎没有出现过下降





## GAN

### 模型架构



尝试可视化一下整体架构(这里不是指网络层 而是两个模型相互的关系)



**AutoEncoder**



基于AutoEncoder 但loss函数不一样 (为什么要这么改)



**Discriminator**



loss计算

 

是怎么想到loss的



### 原理分析



为什么有用(尝试从理论的角度)



### 运行结果



对比AutoEncoder



## Pix2pix



### 模型架构



skip connection是怎么做的

 生成器G用的是Unet结构。

 跳层连接是 ![img](https://cdn.nlark.com/yuque/0/2022/svg/22408135/1647822950580-312bd254-f2d1-4733-a153-923631ee7d2f.svg) 层直接与 ![img](https://cdn.nlark.com/yuque/0/2022/svg/22408135/1647822950567-f664eec1-81da-4567-b512-7cc8f4614da7.svg) 层相加

 

### 原理分析



为什么要skip connection



### 运行结果



### 尝试



- 对结果进行适当的低级图像处理，比如一些滤波操作
- 把图片通道数量改成1。因为我们**主要关注字体变形，不关注颜色**。
- 在原有模型上再加一个unet（和原来模型不一起训练），因为目前的pix2pix形状已经比较形似了，可以尝试单独训练一个unet让成像更清晰的模型。即**一个模型学习形态的变化，另一个模型对结果进行降噪处理**
- 修改网络层的设置。 
- - 当前的训练loss似乎还有降低的空间，或许可以让网络层更深一点?
  - **在网络尾部增加一个softmax**。因为我们看了一下经过生成器前后的图片的灰度直方图，发现映射后的图片的直方图多了很多”杂灰度“，即多了一些半黑不白的地方，而这类灰度是原图片以及目标图片所没有的
- 我们查阅了一下相关资料，很多人说Unet的skip connection的作用主要是在上采样时还原图片的一些空间特征。据了解，resnet中也有一个skip connection，用于缓解网络过深导致的优化困难。这两个skip connection之间存在联系吗？我们的Unet有12层，**是否也需要引进残差学习**?
- 使用cycleGAN
- 损失函数



## cycleGAN
